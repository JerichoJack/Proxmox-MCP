# ğŸ§  MCP Server for Proxmox

The **MCP Server for Proxmox** is a true **real-time automation orchestrator** for your Proxmox environment.  
By ingesting **email** and **WebSocket events** directly from your PVE and PBS nodes, the MCP Server enables instant awareness and automated remediation of issues â€” before you even log in.

This project bridges the gap between **Proxmoxâ€™s event system** and your automation or AI layer, letting MCP Agents act immediately on node, VM, or backup events.

---

## ğŸš€ Why Event Ingestion Is Key

Proxmox generates valuable real-time data through:
- **Email notifications** (alerts, backup results, hardware events)
- **WebSocket events** (VM start/stop, migration progress, UI updates)
- **External integrations** (Gotify notifications, Discord webhooks)

The MCP Server ingests all these sources and provides:
- A **live state view** of all nodes, VMs, and PBS jobs.
- **Structured event forwarding** to MCP Agents for downstream processing.
- **Automated troubleshooting** and remediation via scripts or AI logic.
- **Multi-channel notifications** (Gotify, Discord, email, webhooks).
- A unified bridge between Proxmox's raw signals and your automation stack.

---

## ğŸ§© Architecture Overview

```plaintext
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           MCP Server          â”‚
â”‚ proxmox-mcp-server            â”‚
â”‚                               â”‚
â”‚  â”œâ”€â”€ REST & WebSocket Client  â”‚ â† Subscribes to PVE/PBS events
â”‚  â”œâ”€â”€ Event Ingestion Engine   â”‚ â† Parses, filters, normalizes events
â”‚  â”œâ”€â”€ Event Dispatcher         â”‚ â† Pushes structured events to MCP Agent
â”‚  â”œâ”€â”€ Diagnostic Engine        â”‚ â† Triggers auto-troubleshooting Agents
â”‚  â”œâ”€â”€ Command Tools            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             MCP Agent         â”‚
â”‚  Receives event stream,       â”‚
â”‚  sends notifications (Discord,â”‚
â”‚  Home Assistant, email),      â”‚
â”‚  or invokes troubleshooting.  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§± Project Structure
```plaintext
Proxmox-MCP/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ config.py (MCPConfig)                   âœ… Implemented
â”‚   â”œâ”€â”€ event_dispatcher.py (EventDispatcher)   âœ… Implemented
â”‚   â”œâ”€â”€ event_listener.py (EventListener)       âš™ In Work
â”‚   â”œâ”€â”€ manager.py (MCPManager)                 âœ… Implemented
â”‚   â””â”€â”€ utils.py (logging, helpers)             âœ… Implemented
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ input/                                  # Event ingestion modules
â”‚   â”‚   â”œâ”€â”€ base.py                             # BaseListener class
â”‚   â”‚   â”œâ”€â”€ discord_listener.py                 # DiscordListener âœ… Implemented
â”‚   â”‚   â”œâ”€â”€ email_listener.py                   # EmailListener âœ… Implemented
â”‚   â”‚   â”œâ”€â”€ gotify_listener.py                  # GotifyListener âœ… Implemented  
â”‚   â”‚   â”œâ”€â”€ syslog_listener.py                  # SyslogListener âœ… Implemented
â”‚   â”‚   â””â”€â”€ websocket_listener.py               # WebSocketListener âœ… Standalone & clustered support
â”‚   â”œâ”€â”€ output/                                 # Event dispatch / notification modules
â”‚   â”‚   â”œâ”€â”€ base.py                             # BaseNotifier class
â”‚   â”‚   â”œâ”€â”€ discord_notifier.py                 # DiscordNotifier âœ… Implemented
â”‚   â”‚   â”œâ”€â”€ gotify_notifier.py                  # GotifyNotifier âœ… Implemented
â”‚   â”‚   â”œâ”€â”€ ntfy_notifier.py                    # NtfyNotifier
â”‚   â”‚   â””â”€â”€ email_notifier.py                   # EmailNotifier (future)
â”œâ”€â”€ main.py                                     âœ… Unified Production Entry Point
â”œâ”€â”€ mcp_server.py                              âœ… MCP Server for n8n Integration  
â”œâ”€â”€ n8n_agent_interface.py                     âœ… n8n AI Agent HTTP API (Legacy)
â”œâ”€â”€ Discord ChatBot.json                       âœ… n8n Discord ChatBot Workflow
â”œâ”€â”€ ğŸ¤–Proxmox MCP Agent Workflow - Enhanced.json âœ… Advanced n8n AI Agent Workflow
â”œâ”€â”€ setup_n8n_agent.py                         âœ… Setup & Test Suite
â”œâ”€â”€ .env.example                                âœ… Added
â”œâ”€â”€ requirements.txt                            âœ… Added
â””â”€â”€ README.md                                   âœ… Updated
```

---

## âš™ï¸ Configuration Overview

The `.env` file controls how the MCP Server connects to Proxmox and handles events.  
Each section can be toggled to match your deployment and notification preferences.
- **Standalone, Clustered, or Mixed Lab Support**
- **The key `LAB_CONFIGURATION` allows you to define if your lab consists of standalone nodes, clustered nodes, or a combination.**
- **Event Listeners can be toggled per type (WebSocket, Email, Syslog).**
- **Notifiers can be toggled per backend (Discord, Gotify, Ntfy, Email).**
- **Node credentials, tokens, and host information are stored per-node using .env sections.**
> âš ï¸ **Note:** See the full `.env.example` in the repository for structure and supported keys.

---

## ğŸ—ï¸ Key Features

- **Unified Production Environment** â€“ âœ… Single entry point with `main.py`
- **Comprehensive Testing** â€“ âœ… Full connectivity validation with `--test-connection`
- **MCP Protocol Integration** â€“ âœ… Production-ready MCP server for n8n
- **AI Agent Workflows** â€“ âœ… Complete n8n examples for intelligent automation
- **Multi-Node Support** â€“ âœ… Standalone, clustered, and mixed Proxmox environments
- **Real-time Event Processing** â€“ âœ… WebSocket, Email, Syslog, and API event ingestion
- **Smart Notifications** â€“ âœ… Discord, Gotify integration with intelligent routing
- **Natural Language Interface** â€“ âœ… Discord ChatBot for conversational Proxmox management
- **Approval Workflows** â€“ âœ… AI agents with human oversight for critical operations

---

## ğŸ§° Available MCP Tools

Both the stdio (`mcp_server.py`) and HTTP/WebSocket (`mcp_server_http.py`) servers provide comprehensive tools for managing your Proxmox infrastructure:

### Infrastructure Management
- **`get_available_nodes`** - Get list of all configured PVE and PBS nodes with connection status, version information, and API details
- **`get_cluster_resources`** - Retrieve comprehensive cluster resource information (nodes, VMs, storage)
- **`get_node_status`** - Get detailed status of a specific Proxmox node
- **`get_vm_status`** - Get current status and configuration of a virtual machine or container
- **`get_vm_config`** - Get detailed configuration of a VM/CT

### VM Operations
- **`start_vm`** - Start a virtual machine or container
- **`stop_vm`** - Stop a virtual machine or container
- **`restart_vm`** - Restart a virtual machine or container
- **`create_vm_snapshot`** - Create a snapshot of a VM/CT
- **`list_vm_snapshots`** - List all snapshots for a VM/CT

### Backup Management
- **`list_backups`** - List all available backups for VMs/CTs
- **`create_backup`** - Create a new backup of a VM/CT

### Storage & Network
- **`get_storage_status`** - Get status and usage information for storage
- **`get_network_config`** - Get network configuration for a node or VM

### Task Management
- **`get_task_status`** - Monitor status of asynchronous Proxmox tasks

**Total Tools Available:** 14 comprehensive tools for complete Proxmox management

> ğŸ”„ **Feature Parity:** Both server implementations (stdio and HTTP) provide identical tool functionality, ensuring consistent experience regardless of deployment method.

---

## ğŸš€ Getting Started

### Quick Start (Production Environment)

```bash
# Clone repository and setup
git clone <repository-url>
cd Proxmox-MCP
pip install -r requirements.txt

# Copy and configure environment
cp .env.example .env
# Edit .env with your Proxmox credentials and preferences

# Test all connections
python3 main.py --test-connection

# Start MCP server for n8n integration
python3 main.py --mcp-server
```

### Production Usage

The unified `main.py` serves as the single entry point with two primary modes:

**ğŸ” Connection Testing:**
```bash
python3 main.py --test-connection
```
- Tests all configured PVE/PBS node connectivity
- Validates input/output module functionality
- Verifies MCP server protocol initialization
- Provides comprehensive system health check

**ğŸ”Œ MCP Server Mode (stdio):**
```bash
python3 main.py --mcp-server
```
- Starts enhanced MCP server for n8n integration
- Uses stdio interface for local MCP Client connections
- Comprehensive Proxmox infrastructure management
- Full suite of advanced tools and capabilities

**ğŸŒ MCP HTTP Server Mode (remote):**
```bash
python3 main.py --mcp-http --host 0.0.0.0 --port 8000
```
- Starts enhanced HTTP/WebSocket MCP server
- Supports remote n8n connections via network
- **Same comprehensive functionality as stdio server**
- Ideal for distributed deployments

---

## ğŸ› ï¸ Enhanced MCP Tools & Capabilities

Both the stdio and HTTP MCP servers now provide **identical comprehensive functionality**:

### ğŸ—ï¸ Core Infrastructure Management
- **`get_cluster_status`** - Comprehensive cluster health and resource overview
- **`get_nodes`** - Detailed node information with performance metrics
- **`get_node_status`** - Specific node status including CPU, memory, storage
- **`check_node_health`** - Connectivity and API health validation

### ğŸ–¥ï¸ Virtual Machine Operations
- **`get_vms`** - List VMs with status, configuration, and performance data
- **`execute_vm_command`** - Complete VM lifecycle management:
  - `start`, `stop`, `shutdown` (graceful/forced)
  - `restart`, `reboot`, `suspend`, `resume`, `reset`
  - `status`, `config` - Query current state and configuration

### ğŸ“¦ Container Management
- **`get_lxcs`** - LXC container information and status
- **`execute_lxc_command`** - Full container lifecycle operations:
  - Same command set as VMs with container-specific optimizations

### ğŸ’¾ Storage & Backup Monitoring
- **`get_storage`** - Storage utilization, health, and capacity analysis
- **`get_backup_info`** - Backup job status and scheduling information
- **`get_node_tasks`** - Recent operations, task history, and error tracking

### ğŸŒ Network & Operations
- **`get_network_info`** - Network configuration and interface status
- **`send_notification`** - Multi-channel notifications (Discord, Gotify)

### ğŸ¤– AI-Ready Prompts
- **`proxmox_health_check`** - Guided infrastructure health assessment
- **`proxmox_status_report`** - Comprehensive reporting with recommendations
- **`proxmox_vm_management`** - VM operations best practices
- **`proxmox_storage_analysis`** - Storage optimization guidance
- **`proxmox_incident_response`** - Systematic troubleshooting procedures

---

## ï¿½ n8n AI Agent Integration

This project includes complete n8n workflow examples for intelligent Proxmox management:

### ğŸ¯ Workflow Examples

**1. Discord ChatBot (`Discord ChatBot.json`)**
- Natural language interface for Proxmox queries
- Processes user commands via Discord
- Routes complex requests to specialized AI agents
- Supports both simple queries and complex infrastructure management

**2. Proxmox MCP Agent (`ğŸ¤–Proxmox MCP Agent Workflow - Enhanced.json`)**
- Advanced AI agent for Proxmox infrastructure management
- Uses MCP tools for real-time system monitoring
- Intelligent decision making with user approval workflows
- Comprehensive status reporting and automated remediation

### ğŸ”„ Integration Flow

```plaintext
Discord User â†’ ChatBot Agent â†’ Proxmox MCP Agent â†’ MCP Server â†’ Proxmox Infrastructure
     â†‘                                â†“
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Response & Status Updates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“‹ n8n MCP Client Configuration

There are two ways to connect n8n to your Proxmox MCP Server:

#### Option 1: Local Installation (stdio) - **Same Enhanced Features**

If you can install the MCP server on the same machine as n8n:

```bash
# On your n8n server
git clone <your-repo-url> /opt/Proxmox-MCP
cd /opt/Proxmox-MCP
pip install -r requirements.txt
cp .env.example .env
# Edit .env with your Proxmox server credentials

# Start enhanced stdio server
python3 main.py --mcp-server
```

Configure your MCP Client node:
```json
{
  "parameters": {
    "options": {
      "connection": {
        "type": "stdio",
        "command": "python3",
        "args": ["/opt/Proxmox-MCP/mcp_server.py"],
        "workingDirectory": "/opt/Proxmox-MCP"
      }
    }
  }
}
```

#### Option 2: Remote Server (WebSocket) â­ **Same Enhanced Features**

If your n8n and Proxmox MCP server are on different machines:

```bash
# On your Proxmox MCP server machine
git clone <your-repo-url> /root/Proxmox-MCP
cd /root/Proxmox-MCP
pip install -r requirements.txt
cp .env.example .env
# Edit .env with your Proxmox server credentials

# Start enhanced HTTP/WebSocket server
python3 main.py --mcp-http --host 0.0.0.0 --port 8000
```

Configure your MCP Client node:
```json
{
  "parameters": {
    "options": {
      "connection": {
        "type": "websocket",
        "endpoint": "ws://YOUR-SERVER-IP:8000/ws"
      }
    }
  }
}
```

> ğŸ¯ **Both server modes now provide identical comprehensive functionality** - choose based on your deployment architecture, not feature differences!

**Replace `YOUR-SERVER-IP` with the actual IP address of your MCP server machine.**

#### Connection Examples

**For your current setup (n8n on separate server):**
```json
{
  "nodes": [
    {
      "parameters": {
        "options": {
          "connection": {
            "type": "websocket",
            "endpoint": "ws://192.168.1.100:8000/ws"
          }
        }
      },
      "type": "@n8n/n8n-nodes-langchain.mcpClientTool",
      "typeVersion": 1.2,
      "position": [1200, 592],
      "id": "bcc9b8b5-9b94-4f73-81de-ea00dd4c9ba6",
      "name": "MCP Client"
    }
  ]
}
```

#### Server Management Commands

**Start enhanced stdio server (local n8n):**
```bash
python3 main.py --mcp-server
```
*Full feature set: 13 tools + 5 AI prompts + resources*

**Start enhanced HTTP server (remote n8n):**
```bash
python3 main.py --mcp-http --host 0.0.0.0 --port 8000
```
*Identical feature set: 13 tools + 5 AI prompts + resources*

**Test connectivity and validate setup:**
```bash
python3 main.py --test-connection
```

**Health check (HTTP server):**
```bash
curl http://YOUR-SERVER-IP:8000/health
```

> âœ¨ **Feature Parity Achieved**: Both servers now provide identical comprehensive Proxmox management capabilities!

## ğŸ§© Enhanced Use Cases

### ğŸ¤– AI-Powered Infrastructure Management
- **Intelligent Health Monitoring**: AI agents continuously assess cluster health using `check_node_health`
- **Predictive Maintenance**: Automated analysis of `get_node_tasks` to identify potential issues
- **Smart Resource Allocation**: AI-driven VM/LXC management using comprehensive `get_vms`/`get_lxcs` data

### ğŸ“Š Advanced Operations & Analytics  
- **Capacity Planning**: AI analysis of `get_storage` trends for expansion recommendations
- **Performance Optimization**: Intelligent VM placement using detailed node resource data
- **Backup Strategy**: AI-enhanced backup monitoring via `get_backup_info` with failure prediction

### ğŸ”” Intelligent Notification & Response
- **Context-Aware Alerts**: Smart notifications using severity analysis and historical data
- **Natural Language Interface**: Discord ChatBot answers complex queries like "Which VMs need attention?"
- **Automated Remediation**: AI agents execute safe recovery procedures with human approval

### ğŸŒ Cross-Platform Integration
- **Multi-Channel Communications**: Unified notifications across Discord, Gotify, and other platforms
- **Event-Driven Workflows**: Automatic responses to Proxmox events with intelligent escalation
- **Distributed Management**: Remote HTTP server enables management from anywhere

---

## ğŸ—ï¸ Setup & Installation

## ğŸ“¦ requirements.txt (recommended)

```
mcp
proxmoxer
requests
aiohttp
python-dotenv
configparser
apscheduler
```

- Python 3.10+  
- `pip` or `uv` for dependency management  
- Proxmox VE environment with API access or event hooks enabled

### Clone the Repository

```bash
git clone https://github.com/JerichoJack/Proxmox-MCP.git
cd Proxmox-MCP
```

### Install Dependencies

```bash
pip install -r requirements.txt
```

---

### ğŸ§© Proxmox API Setup

To allow the MCP Server to receive data and interact with your Proxmox environment, youâ€™ll need to create a dedicated **API user** with restricted permissions.  
This process should be repeated for both **Proxmox VE (PVE)** and **Proxmox Backup Server (PBS)** if youâ€™re using both.

---

#### ğŸ§  Step 1: Create a Dedicated API User (PVE)

1. Log in to your **Proxmox VE Web UI**.
2. Navigate to **Datacenter â†’ Permissions â†’ Users**.
3. Click **Add** â†’ **User**.
4. Enter a username, for example:
   ```
   mcp@pve
   ```
5. Choose **Realm:** `Proxmox VE authentication server` (or another if you use LDAP).
6. Set a **secure password** and click **Add**.

---

#### ğŸ§© Step 2: Create an API Token (Optional but Recommended)

If you prefer **token-based authentication** (safer for services):

1. Navigate to **Datacenter â†’ Permissions â†’ API Tokens**.
2. Click **Add**.
3. Choose the user you just created (`mcp@pve`).
4. Give the token an ID, e.g. `mcp-token`.
5. Uncheck **Privilege Separation** if your MCP server needs full access to the assigned roles.
6. Copy the **Token ID** and **Secret** â€” youâ€™ll need these for your `.env` file.

---

#### ğŸ” Step 3: Assign Permissions

1. Go to **Datacenter â†’ Permissions â†’ Add â†’ User Permission**.
2. Choose:
   - **Path:** `/` (or restrict to a specific node if you prefer)
   - **User:** `mcp@pve`
   - **Role:** `PVEAuditor` *(read-only)*  
     or `PVEAdmin` *(for full event and control access)*.
3. Click **Add**.

---

> âš ï¸ **Note:** If your MCP server runs on the same host as PVE, you can use `localhost` for the host address.  
> Ensure the MCP process user has network access to the Proxmox API port (default `8006`).

---

### ğŸ—„ï¸ Proxmox Backup Server (PBS) API Setup

If you also want MCP to listen for backup-related events from your **Proxmox Backup Server**, create a similar user there.

#### Step 1: Create a User on PBS

1. Log in to your **PBS Web UI**.
2. Go to **Access Control â†’ Users** â†’ **Add**.
3. Create a user:
   ```
   mcp@pbs
   ```
4. Set a secure password.

#### Step 2: Assign Role and Permissions

1. Go to **Access Control â†’ Permissions â†’ Add**.
2. Choose:
   - **Path:** `/`  
   - **User:** `mcp@pbs`  
   - **Role:** `Audit` *(read-only)* or `Admin` *(full access)*.

#### Step 3: (Optional) Create an API Token

Similar to PVE:
- Go to **Access Control â†’ API Tokens**.
- Click **Add**.
- Create token for `mcp@pbs`.
- Copy the token and secret for `.env`.

---

### âœ… Final Step: Test Connectivity

After configuring your `.env` file, validate your setup with the built-in connection tester:

```bash
# Test all configured nodes and IO modules
python main.py --test-connection
```

**What `--test-connection` validates:**
- âœ… **PVE/PBS API Access** â€“ Tests credentials and connectivity for each configured node
- âœ… **WebSocket Endpoints** â€“ Verifies subscription capability to live event streams  
- âœ… **Input Listeners** â€“ Tests Gotify stream, Discord webhook, and Syslog port binding
- âœ… **Output Notifiers** â€“ Sends test notifications via Gotify and Discord (if enabled)
- âœ… **Configuration Parsing** â€“ Validates `.env` structure and required parameters

**Expected Output:**
```
ğŸ”— Testing connectivity to configured nodes...
âœ… PVE Node 'pve-main' connectivity test passed
âœ… PBS Node 'pbs-main' connectivity test passed
ğŸ”” Testing Gotify output notifier...
âœ… Gotify output test sent successfully
ğŸ”” Testing Gotify input listener (stream)...
âœ… Gotify input test stream succeeded

ğŸ‰ All nodes and IO modules are reachable and ready!
```

Once all tests pass, you can start the full MCP server:

```bash
# Start MCP Server with real-time event processing
python main.py
```

---

## ğŸ”” Notification System

* [x] **Email** â€“ reads Proxmox email alerts
* [x] **WebSocket** â€“ subscribes to live Proxmox event streams
* [x] **Syslog** â€“ listens for UDP syslog messages from Proxmox nodes
* [x] **Gotify** â€“ âœ… Full input/output implementation with streaming and notifications
* [x] **Discord** â€“ âœ… Full input/output implementation with webhooks and rich embeds
* [âš™] **Ntfy** â€“ planned

> All notification backends use a unified interface; adding a new notifier only requires implementing `.send(title, message)`.

---

## ğŸ› ï¸ Troubleshooting

### Common Issues

**âŒ Configuration file '.env' not found**
```bash
# Copy the example configuration
cp .env.example .env
# Edit with your Proxmox credentials
nano .env
```

**âŒ MCP Server connection issues in n8n**
- Ensure working directory is set to the Proxmox-MCP folder
- Use absolute paths for the Python command
- Verify python3 is in your PATH: `which python3`
- Test manually: `python3 mcp_server.py` (should show connection info)

**âŒ Some connectivity tests failed**
- Check network connectivity to Proxmox nodes
- Verify API credentials in `.env`
- Ensure firewall allows connections to required ports
- Check Discord/Gotify tokens and webhook URLs

**âŒ n8n workflow execution errors**
- Ensure MCP server is running: `python3 main.py --mcp-server`
- Check n8n MCP Client node configuration
- Verify channelId is being passed correctly from Discord
- Test individual MCP tools in n8n test mode

### Getting Help

1. **Run diagnostics**: `python3 main.py --test-connection`
2. **Check logs**: Look for error messages in terminal output
3. **Validate configuration**: Review `.env` file settings
4. **Test components individually**: Use the setup script for targeted testing

---

## ğŸ¤ Contributing

Contributions welcome! Open an issue or submit a pull request.
Discuss major changes in the Issues section first.

---

## ğŸ“„ License

MIT License

---

> **MCP Server for Proxmox** â€” A real-time automation orchestrator bringing intelligence and autonomy to your Proxmox infrastructure.

